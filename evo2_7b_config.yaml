# mostly a copy of https://github.com/Zymrael/savanna/blob/main/configs/7b-ft/model_configs/7b-1M-ecoli-ft.yml
# -------- Base 7b 1M ft template -------- #

# Hotfix PR 146: consistency of decay in hyena medium explicit filters
explicit_filter_num_decay_repeats: 1

# Inherited from
# https://github.com/Zymrael/savanna/blob/main/configs/7b-ft/model_configs/7b-10K-phage-ft.yml
# for finetuning; essential for right padding to work. That is, if you
# enforced a sample length during the data processing step, it's essential to mask
# padding and EOD token loss by turning these flags below on.
# pad_mask_loss: True
# eod_mask_loss: True

# Needed for MP32
pad_mlp_weights: true

# CP Config
use_cp_flash_te: true
te_attn_backend: 'FLASH'
use_cp_ring: false
use_cp_hyena: true

# Checkpointing
load: 'ckpt/evo2_7b_context_reduced' # your base checkpoint directory
use_checkpoint_lr_scheduler: false
use_checkpoint_num_samples: false
finetune: true

checkpoint-factor: 250 # checkpoint every 250 steps; feel free to tweak
keep-last-n-checkpoints: 10
save_retain_interval: 200000 # Disable upload to S3 as it destablizes training
async_save: false

# Per ds evals
# num_workers: 1 # need to set this to 1 for world_size >= 32, else CPU OOM during dataloading
# do_per_ds_valid: true
# eval_per_ds_interval: 500
# eval_per_ds_iters: 1

# Context ext params
# You can play with the sequence length by tuning max_position_embeddings
max_position_embeddings: 4096

# You can play with the sequence length
seq_length: 4096
pos_emb: 'rotary_linear_scaled'
rotary_emb_base: 100000000000
rotary_emb_scaling_factor: 128
rotary_pct: 1

# Parallelism
# Tweak model parallelism knobs before touching pipeline parallelism; for Evo 2
# 7B on H100s, all of these flags can be 1
pipe_parallel_size: 1
model_parallel_size: 1
context_parallel_size: 1

# sp and pa are mutually exclusive
sequence_parallel: true
partition-activations: false

zero_optimization:
  stage: 1
  allgather_partitions: true
  allgather_bucket_size: 500000000
  overlap_comm: true
  reduce_scatter: true
  reduce_bucket_size: 500000000
  contiguous_gradients: true
  cpu_offload: false

# I recommend tuning batch size; remember that effective batch size (num of GPUs * 
# train_micro_batch_size_per_gpu * gradient_accumulation_steps) should be 
# significantly higher than 1 (ideally 16-64) for effective training. 
# The intuition is that for each learning step, you want as accurate an estimate as
# possible of the gradient update over the entire training dataset. Since that's 
# computationally infeasible, the next best thing is sampling as many datapoints 
# from the training batch as possible (i.e., increasing the effective per-step batch 
# size). 
train_micro_batch_size_per_gpu: 2
gradient_accumulation_steps: 4

checkpoint-activations: true
checkpoint-num-layers: 1

# Optimizer settings
# Tune train-iters based on token count of FT dataset and world size
train-iters: 120000 
lr-decay-iters: 4000
optimizer:
  type: 'Adam'
  params:
    lr: 0.00005
    betas: [0.9, 0.95]
    eps: 1.0e-8
min_lr: 0.000005
warmup: 0.05

# Loss
to_upper: 'normalized_weighted'
mask_loss_control_tags: true
lowercase_loss_reweighting: 0.1

#Logging
log_memory_stats: true

# --------------------------------- #
# Common across all context lengths

# cgcg config
use_cgcg: false
use_cgcg_short: false
use_cgcg_mlp: false

cgcg_dtype: 'bfloat16'

# Model Config
make_vocab_size_divisible_by: 8

num_layers: 32
hidden_size: 4096
num_groups_hyena: 4096
num_groups_hyena_medium: 256
num_groups_hyena_short: 256
num_groups_hyena_mlp: 256
num_attention_heads: 32

operator-config:
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['flash_te'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['flash_te'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['flash_te'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['flash_te'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['hyena_se'], 1]
  - [['hyena_mr'], 1]
  - [['hyena'], 1]
  - [['flash_te'], 1]

hyena_mr_len: 128 # default is null
log_attn_norms: false

prenorm: true
postnorm: false
pre_mlp_norm: true
outer_mlp_norm: false
no_weight_tying: false
gpt_j_residual: false
normalize_hyena_filters: false
short-conv-L: 3
hyena_filter_fast_decay: 0.3
hyena_filter_slow_decay: 1.2
hyena_filter_w: 14
hyena_filter_cls: 'implicit_modal'
hyena_medium_filter_cls: 'explicit_single_decay'
explicit_filter_decay_preset: 'weak'
hyena_filter_order: 16
hyena_filter_wd: 0.0
use_fast_heads: false
use_slow_heads: false
use-hyena-filter: true
output_layer_parallelism: 'column'
bias_dropout_fusion: false
norm: 'rmsnorm'
rms_norm_epsilon: 1.0e-6
identity_mlp: false
mlp_type: 'llama'
scaled-upper-triang-masked-softmax-fusion: true
bias-gelu-fusion: false
init_method: 'small_init'
output_layer_init_method: 'wang_init'

data-impl: 'mmap'
synchronize-each-layer: false
gradient_clipping: 1.0
weight-decay: 0.1
hidden-dropout: 0.0
attention-dropout: 0.0
precision: 'bfloat16'
bf16:
  enabled: true
distributed-backend: 'nccl'
lr-decay-style: 'cosine'
eval-interval: 2000
eval-iters: 20
log-interval: 5
steps_per_print: 5

wall_clock_breakdown: false
tokenizer_type: 'CharLevelTokenizer'
use_fp8_input_projections: true
use_fp8_output_projections: true
use_fp8_mlp_projections: true
use_fp8_norm: true

checkpoint_strict_load: false # can flip
make_gated_mlp_multiple_of: 512 # debugs a shape mismatch
materialize_attn_mask: false # default false, to save memory
fast_conv_proj: true
hyena_se_len: 7
hyena_mlp_len: 7

# copied from https://github.com/Zymrael/savanna/blob/main/configs/7b-ft/model_configs/7b-10K-phage-ft.yml
save:
  /path/to/your/save/directory
wandb_group: finnatune
wandb_run_name: first-finnatune
expandable_segments: true
recycle_events: false
